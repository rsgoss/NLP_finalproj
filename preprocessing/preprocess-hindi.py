import pandas as pd
import numpy as np
import csv
import re
from conllu import parse
from pathlib import Path
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem.snowball import SnowballStemmer
from alive_progress import alive_bar
import nltk
# from emo_unicode import __all_emo__
import emoji_data_python

# files to process
# files = ['val_3k_split', 'test_unalbelled']
files = ['train_14k_split', 'val_3k_split']

# Stopwords for English and Hindi
hin_stop_words = pd.read_csv('hin_stop_words.txt', sep="\t", header=None)
hin_stop_words = list(hin_stop_words[0])[:1000]
stop_words = set(stopwords.words('english'))

# variables
snowBallStemmer = SnowballStemmer("english")

symbol_dict = { 'â¤': 'love', 'â™¥': 'love', 'â¤â¤': 'love', 'â™¥â™¥': 'love', 'Ã¹Â§': 'love', 'â€šÃ´â€¢': 'love', 'â˜º': 'smile'}

punctuation1 = ['â€¦', '.', '!', '#', '__', '?', '!', '&', '@', '$', '%', '^', '*', '+', '=', ':', ';', ',', '/',
                '[', ']', '{', '}', '|', '_', '`', '~', 'âœ–', 'â•®', 'â•­', 'ðŸ”', 'Âº', 'Î´']

punctuation2 = ['RT', '_', '.', 'â€¦', '...', '!', '#', '__', '___', '____', '_____', '..', '....', '.....', '.â€¦', '.....',
               '@___', '@_', '.........', '.......', '((', '(((', 'â€”â€¦', '........', "'", ',', '|', '?', '!', '......',
               '!!', '??', 'â€¦', ' ................', '.....................â€¦', 'â€™', '!!!', '!!!!', '!!!!!', '!!!!!!',
               '!!!!!!!', '!!!!!!!!', '!!!!!!!!!', '!!!!!!!!!!', '!!!!!!!!!!!', '!!!!!!!!!!!!', '??', 'â€”', '???'
               '*', '**', '***', '****', '*****', '******', '*******', '********', '*********', '**********', '&', '-',
               '>', '<', 'â€', 'â€œ', '"', '.......................', 'â–º']

symbol = ['Ã°Å¸â„¢ÂÃ°Å¸Å’Â¹', 'Ã°Å¸â„¢ÂÃ°Å¸Å’Â¹', 'Ã‚Â´', 'Ã°Å¸ËœÂÃ°Å¸ËœÂÃ°Å¸ËœÂ', '?Ã°Å¸Ëœâ€šÃ°Å¸Ëœâ€šÃ°Å¸Ëœâ€š', '##', '#Ã Â¤Å¡Ã Â¤Â®Ã Â¥ÂÃ Â¤ÂªÃ Â¤Â¾Ã Â¤Â°Ã Â¤Â£', 'Ã°Å¸Å½â€šÃ°Å¸â„¢ÂÃ¢ÂÂ¤Ã¯Â¸Â',
     'Ã°Å¸ÂÂ', 'Ã°Å¸â€™Æ’', 'Ã°Å¸Â¤Â®', 'Ã¢ÂÂ¤+Ã°Å¸â€Â¨=Ã°Å¸â€™â€', 'Ã™Ë†Ã˜Â±Ã˜Â¯Ã›Å’', '#ÃªÂµÂ¬Ã«â€¦Â¸', 'Ã°Å¸ËœÂ­Ã°Å¸â€™â€“Ã°Å¸â€™â€“', 'Ã¢ÂÂ¤Ã¢ÂÂ¤Ã¢ÂÂ¤', 'Ã°Å¸Å½â€š', 'Ã°Å¸â€“ÂÃ°Å¸ÂÂ½',
     'Ã¢ÂÂ¤', 'Ã°Å¸ËœÂÃ°Å¸â€˜ÂÃ°Å¸â€™Â', '..Ã°Å¸â„¢â€¡Ã¢â‚¬ÂÃ¢â„¢â‚¬Ã¯Â¸Â', 'Ã°Å¸Ëœâ‚¬Ã°Å¸Ëœâ‚¬Ã°Å¸Ëœâ‚¬Ã°Å¸Ëœâ‚¬Ã°Å¸Ëœâ€¦Ã°Å¸Ëœâ‚¬Ã°Å¸Ëœâ€¦Ã°Å¸Ëœâ‚¬Ã°Å¸Ëœâ€¦', 'Ã°Å¸ËœÂÃ°Å¸ËœÂ', 'Ã°Å¸â€˜Å½Ã°Å¸â€“ÂÃ°Å¸â€¡ÂµÃ°Å¸â€¡Â°',
     'Ã°Å¸Â§Â¡', 'Ã°Å¸ËœÂ©Ã¢ÂÂ¤Ã¯Â¸Â', 'Ã°Å¸Å½Å¸', 'Ã°Å¸Â¤â€”Ã°Å¸Â¤â€”Ã°Å¸Â¤â€”Ã°Å¸â€™ÂÃ°Å¸â€™Â', 'Ã°Å¸â„¢ÂÃ°Å¸â„¢â€š', 'Ã°Å¸â€Â¥Ã°Å¸â€Â¥', 'Ã°Å¸Ââ€ ', 'Ã°Å¸Å’Â´', 'Ã°Å¸â€™Â¯', 'Ã°Å¸ËœÂª',
     'Ã Â¤Â¬Ã Â¤Â¹Ã Â¥ÂÃ Â¤Â¤', 'Ã°Å¸â€˜Â¶', 'Ã°Å¸â€˜â€¡Ã°Å¸â€˜â€¡', 'Ã°Å¸Å½Â§', 'Ã£â€¦â€¹Ã£â€¦â€¹Ã£â€¦â€¹Ã£â€¦â€¹', 'Ã¢Å“Â¨', 'Ã°Å¸Â¤Â­Ã°Å¸Â¤Â­', 'Ã°Å¸ËœÂ¡Ã°Å¸Â¤Â¬Ã°Å¸Â¤Â¬', 'Ã˜Â¯ÃšÂ¾Ã˜Â´Ã˜ÂªÃšÂ¯Ã˜Â±Ã˜Â¯Ã›Å’',
     'Ã°Å¸â€˜â€°', 'Ã°Å¸Ëœâ‚¬Ã°Å¸Ëœâ‚¬', 'Ã¢â‚¬Â¢', '.Ã¢â‚¬Â¦', 'Ã¢â„¢Â¥Ã¯Â¸ÂÃ¢â„¢Â¥Ã¯Â¸Â', 'Ã°Å¸Å’Âº', 'Ã°Å¸Å’Â¸Ã°Å¸â€“Â¤Ã°Å¸â€™Â¥', 'Ã¢â€ â€˜', 'Ã°Å¸ËœÂÃ°Å¸â€™â€º', 'Ã°Å¸â€™Å“Ã¢ÂÂ¤', '~Ã°Å¸â„¢Â',
     '..Ã°Å¸Ëœâ€šÃ°Å¸Ëœâ€š', 'Ã°Å¸â€™â„¢', 'Ã°Å¸â€™Â«Ã°Å¸â€™Â«', 'Ã°Å¸â„¢ÂÃ¢ÂÂ¤Ã¯Â¸ÂÃ¢â‚¬Â¦', 'Ã°Å¸â€™Âª', 'Ã«Â°Â¤Ã«Â°â€Ã«Å¾Å’', '~', 'ÃƒÂ', 'Ã°Å¸â€™â€¢Ã°Å¸â€™Â«', 'Ã°Å¸â€˜â€¡',
     'Ã°Å¸Ëœâ€šÃ°Å¸Ëœâ€šÃ°Å¸ËœÂ¤', 'Ã Â¤Â¬Ã Â¤Â§Ã Â¤Â¾Ã Â¤Ë†', 'Ã°Å¸Ëœâ€š', '...Ã¢ËœÂÃ¯Â¸ÂÃ¢ËœÂÃ¯Â¸Â', 'Ã°Å¸Â¤ÂÃ°Å¸ËœÅ¸', 'Ã°Å¸Â¤â€', 'Ã°Å¸ËœËœÃ°Å¸ËœËœÃ°Å¸ËœËœÃ°Å¸ËœËœÃ°Å¸ËœËœÃ°Å¸ËœËœÃ°Å¸ËœËœÃ°Å¸ËœËœ',
     'Ã°Å¸Â¥Â°', 'Ã°Å¸Â¥Â°Ã°Å¸â€™â€ºÃ°Å¸â€™â€ºÃ°Å¸â€™â€ºÃ°Å¸â€™â€ºÃ°Å¸â€™â€º', '.@', 'Ã°Å¸â„¢ÂÃ°Å¸ÂÂ»Ã°Å¸â„¢ÂÃ°Å¸ÂÂ»Ã°Å¸â„¢ÂÃ°Å¸ÂÂ»Ã°Å¸â„¢ÂÃ°Å¸ÂÂ»Ã°Å¸â„¢ÂÃ°Å¸ÂÂ»', 'Ã‚', 'Ã°Å¸Ëœâ€¦Ã°Å¸Ëœâ€¦Ã°Å¸Â¤Â£Ã°Å¸Â¤Â£',
     'Ã°Å¸ËœÅ’', 'Ã Â¤Â­Ã Â¤ÂµÃ Â¤Â¿Ã Â¤Â·Ã Â¥ÂÃ¢â‚¬Â¦', 'Ã°Å¸â„¢â€ Ã°Å¸ËœÂ«Ã°Å¸Ëœâ€šÃ°Å¸Ëœâ€™', 'Ã°Å¸â€™â‚¬', ';;;;;;;', 'Ã°Å¸ËœÅ“Ã°Å¸ËœÅ“', 'Ã Â¨Â¸Ã Â¨Â¾Ã Â¨Â²Ã Â¨Â¾Ã Â¨Â¹Ã Â©â‚¬Ã Â¨Â',
     'Ã°Å¸ËœÂ­Ã¢ÂÂ¤Ã¯Â¸Â', 'Ã°Å¸Â¥ÂºÃ¢â‚¬Â¦', 'Ã°Å¸ËœÅ Ã°Å¸â€˜Â', 'Ã°Å¸ËœÂ­Ã°Å¸Ëœâ€šÃ°Å¸Ëœâ€šÃ°Å¸ËœÂ­Ã°Å¸Ëœâ€šÃ°Å¸Ëœâ€šÃ°Å¸Ëœâ€šÃ°Å¸ËœÂ­Ã°Å¸ËœÂ­Ã°Å¸ËœÂ­Ã°Å¸ËœÂ­Ã°Å¸ËœÂ­Ã°Å¸ËœÂ­Ã°Å¸ËœÂ­Ã°Å¸ËœÂ­Ã°Å¸ËœÂ­Ã°Å¸ËœÂ­',
     'Ã°Å¸Ëœâ€šÃ°Å¸ËœÂ­', '..!Ã°Å¸Ëœâ€š', '!Ã°Å¸ËœÅ’', 'Ã°Å¸Â¤Â¦', '...Ã°Å¸ËœÂ¤', 'Ã¬â€ºÅ’Ã«Ë†â€ž', 'Ã°Å¸Ëœâ€™Ã°Å¸Ëœâ€™Ã°Å¸Ëœâ€™', 'Ã°Å¸â„¢Â', 'Ã¢ÂÂ¤Ã¯Â¸Â.', 'Ã¢â„¢Â¥Ã¯Â¸Â',
     '#Ã™â€žÃ›Å’Ã™â€žÃ›Æ’_Ã˜Â§Ã™â€žÃ™â€šÃ˜Â¯Ã˜Â±', 'Ã°Å¸Ëœ', 'Ã°Å¸Ëœâ€°Ã°Å¸ËœÆ’', ').Ã¢â‚¬Â¦', '.Ã°Å¸Â¤â€”Ã°Å¸Â¤â€”', '...Ã¢â‚¬Â¦', 'Ã°Å¸Ëœâ€žÃ°Å¸Ëœâ€žÃ°Å¸Å½Â¤', 'Ã°Å¸ËœÅ’.', '??Ã°Å¸Ëœâ€š',
     'Ã°Å¸â€™Å¾Ã°Å¸Å½â€š', 'Ã°Å¸â„¢Å’Ã°Å¸â€™â€¢Ã°Å¸â€™â€¢', 'Ã°Å¸ËœÂ­Ã°Å¸Ëœâ€š', 'Ã°Å¸Â¥Â´', 'Ã Â¤Â®Ã Â¥â€¡Ã Â¤Â¹Ã Â¤Â¨Ã Â¤Â¤', 'Ã°Å¸â€™Å¾Ã°Å¸â€™â€œÃ°Å¸â€™â€”Ã°Å¸â€™â€“Ã°Å¸â€™Ëœ', 'Ã°Å¸â€™ÂªÃ°Å¸ËœÅ½', 'Ã Â¤Â°Ã Â¤Â¹Ã Â¤Â¾',
     'Ã°Å¸ËœÅ¾', 'Ã°Å¸Ëœâ€šÃ°Å¸â€¡Â¨Ã°Å¸â€¡Â¦', '!Ã°Å¸ËœÂ', 'Ã°Å¸â€™â€“Ã°Å¸â€™â€¢', 'Ã¢ÂÂ¤Ã¯Â¸ÂÃ¢ÂÂ¤Ã¯Â¸ÂÃ¢ÂÂ¤Ã¯Â¸Â', 'Ã°Å¸ËœÂ»Ã°Å¸ËœÂ»Ã°Å¸â€˜ÂÃ°Å¸â€˜Â', 'Ã¢â‚¬Â', '!Ã°Å¸â€˜Â', 'Ã°Å¸Å’Â¹',
     'Ã°Å¸Â¤Â£Ã°Å¸Â¤Â£Ã°Å¸Â¤Â£Ã°Å¸Â¤Â£', 'Ã¢â€“Â¶Ã¯Â¸Â', 'Ã¢ËœÂÃ°Å¸ÂÂ»', 'Ã°Å¸Ëœâ€°Ã°Å¸ËœÅ“', 'Ã°Å¸Å’Â³', 'Ã°Å¸Ëœâ€¦', 'Ã°Å¸Â¤Â£', 'Ã¢â‚¬â„¢.', 'Ã°Å¸Å’Âµ', 'Ã°Å¸â€˜Å½Ã°Å¸â€˜Å½',
     'Ã°Å¸â€™â€œÃ°Å¸â€™â€œÃ°Å¸ËœÂÃ°Å¸â€˜Â­', 'Ã°Å¸ËœÅ“Ã°Å¸ËœÅ“Ã°Å¸ËœÆ’Ã°Å¸ËœÆ’', '...Ã°Å¸ËœÂ¹', 'Ã°Å¸ËœÂ¡', 'Ã°Å¸Å’Â¸', 'Ã°Å¸ËœÂ', 'ÃƒÂ©', '..Ã°Å¸ËœËœÃ°Å¸Â¤ËœÃ°Å¸â€˜Â', 'Ã¢â€ºÂ½Ã¯Â¸Â',
     'Ã°Å¸â€™ÂÃ°Å¸â€™ÂÃ°Å¸â€™ÂÃ°Å¸â„¢ÂÃ°Å¸â„¢Â', 'Ã°Å¸Ëœâ€šÃ°Å¸ËœÂÃ¢ÂÂ¤Ã°Å¸â„¢Â', 'Ã°Å¸â„¢â€ž', '#Ã Â¤Â¶Ã Â¥â€¡Ã Â¤Â°', 'Ã°Å¸Ëœâ€™Ã°Å¸Ëœâ€™Ã°Å¸Ëœâ€™Ã°Å¸Ëœâ€™', 'Ã°Å¸Ëœâ€¦Ã°Å¸Ëœâ€¦Ã°Å¸Ëœâ€¦Ã°Å¸Ëœâ€¦',
     '#Ã§Å½â€¹Ã¥Ââ€ºÃ¨Â±Âª', 'Ã°Å¸â„¢Å Ã°Å¸Ëœâ€šÃ°Å¸Ëœâ€ Ã°Å¸Ëœâ€ ', 'Ã°Å¸ËœÂÃ°Å¸ËœÂÃ°Å¸ËœÂÃ°Å¸ËœÂÃ°Å¸â€Â¥', 'Ã˜Â§Ã˜Â³ÃšÂ©Ã›â€™', '.Ã°Å¸ËœÂ¸', ')Ã¢â‚¬Â¦', 'Ã Â¤Â²Ã Â¥â€¡Ã Â¤Å¸Ã°Å¸Â¤â€˜',
     '..Ã°Å¸Ëœâ€¹Ã°Å¸Â¤â€œÃ°Å¸ËœÅ½', 'Ã¢Å¸Â¶', 'Ã°Å¸Ââ€¢', '8Ã¢â‚¬Â¦', 'Ã°Å¸â€¡Â®Ã°Å¸â€¡Â³', 'Ã°Å¸Ëœâ€ Ã°Å¸Ëœâ€ ', 'Ã°Å¸Â¥Â°Ã°Å¸Â¥Â°Ã°Å¸Â¥Â°', 'Ã°Å¸Ëœâ€šÃ°Å¸â€™Å“', 'Ã°Å¸â€™ÂªÃ°Å¸â„¢Å’Ã¢â„¢Â¥Ã¯Â¸Â',
     'Ã°Å¸ËœÂ­Ã°Å¸â€™â€”Ã°Å¸â€™â€”', 'Ã°Å¸â„¢ÂÃ°Å¸ÂÂ¾', 'Ã°Å¸Â¥ÂºÃ°Å¸Â¥Âº', 'Ã¬Â¡Â´Ã«â€šËœÃ¬â€ºÆ’ÃªÂ²Â¨', 'Ã°Å¸â„¢ÂÃ°Å¸ÂÂ¼.', '?Ã°Å¸ËœÂ¬', 'Ã Â¨Â¸Ã Â©â€¹', 'Ã°Å¸â€™Æ’Ã°Å¸ÂÂ¾Ã°Å¸â€™Æ’Ã°Å¸ÂÂ¾',
     'Ã°Å¸ËœÅ½Ã°Å¸â€¡ÂµÃ°Å¸â€¡Â°Ã°Å¸ËœÅ½', 'Ã°Å¸Â¥Â°Ã°Å¸Â¥Â°Ã¢ÂÂ¤Ã¢ÂÂ¤Ã°Å¸Å’Â¹Ã°Å¸Å’Â¹', 'Ã¢â‚¬Â¦', 'Ã°Å¸ËœÅ ', 'Ã¢Ââ€”Ã¢Ââ€”', 'Ã°Å¸ËœÂÃ°Å¸ËœÂ', '!!Ã°Å¸â€™Å“', 'Ã°Å¸â€¡ÂµÃ°Å¸â€¡Â°Ã¢ÂÂ¤Ã°Å¸â€¡Â®Ã°Å¸â€¡Â³',
     'Ã Â¤Â°Ã Â¤â€šÃ Â¤â€”', 'Ã¢Â­ÂÃ¢Â­ÂÃ¢Â­ÂÃ¢Â­ÂÃ¢Â­ÂÃ¢Â­ÂÃ¢â‚¬Â¦', 'Ã°Å¸â€“Â¤Ã¢ÂÂ¤Ã¯Â¸Â', 'Ã°Å¸ÂÂªÃ°Å¸Â¥Â§', 'Ã Â¤â€¢Ã Â¤Â¾', 'Ã°Å¸Å½â€°Ã°Å¸Å½â€°Ã°Å¸ËœÅ’Ã°Å¸ËœÅ’', 'Ã°Å¸ËœÂ­Ã°Å¸ËœÂ­Ã°Å¸Ëœâ€š',
     'Ã°Å¸ËœÅ“', 'Ã°Å¸â€˜Å’Ã°Å¸ËœÂÃ°Å¸â€™â€¢Ã°Å¸â€™â€œÃ°Å¸Å½Â¤Ã°Å¸â€˜ÂÃ°Å¸â€˜ÂÃ°Å¸â€˜ÂÃ°Å¸â€˜ÂÃ°Å¸Å’Â¹Ã°Å¸Å’Â¹Ã°Å¸Å’Â¹Ã°Å¸Å’Â¹Ã°Å¸Å’Â¹Ã°Å¸Å’Â¹', 'Ã°Å¸ËœÂ­Ã°Å¸ËœÂÃ°Å¸â€Â¥', 'Ã°Å¸Â¤ÂªÃ°Å¸Â¤ÂªÃ°Å¸Â¤Âª', 'Ã°Å¸Â¤â€Ã°Å¸Â¤â€',
     'Ã°Å¸â€˜ÂÃ°Å¸â€˜Â', 'Ã°Å¸Ëœâ€šÃ°Å¸Ëœâ€šÃ°Å¸Ëœâ€šÃ°Å¸â„¢â€ž', 'Ã°Å¸Ëœâ€°Ã°Å¸â€˜Â£Ã°Å¸Å½Ë†', 'Ã°Å¸ËœÂ·', 'Ã°Å¸ËœËœÃ°Å¸ËœËœ', 'Ãšâ€ ', 'Ã Â¤Â¸Ã Â¤ÂµÃ Â¤Â¤Ã Â¥â‚¬Ã Â¤Â¨', 'Ã°Å¸ÂÂ¬', 'Ã°Å¸â€™Â¯%',
     'Ã°Å¸ËœÂ', 'Ãƒâ€”Ãƒâ€”Ãƒâ€”', 'Ã°Å¸â€™ÂÃ°Å¸â€™Â', '!!!Ã¢â‚¬Â¦', 'Ã°Å¸ËœÂ´Ã¢Å“Â¨', 'Ã¢â„¢Â¥', '#Ã¢â‚¬Â¦', '..Ã°Å¸Â¤Â­', 'Ã°Å¸Ëœâ€šÃ°Å¸Ëœâ€š.', 'Ã°Å¸Ëœâ‚¬Ã°Å¸â€˜ÂÃ°Å¸â€˜ÂÃ°Å¸â€˜ÂÃ°Å¸â€˜Â',
     '.......Ã°Å¸â„¢Â', 'Ã¢â‚¬ÂÃ°Å¸Ëœâ€šÃ°Å¸Ëœâ€š', 'Ã°Å¸Â¤Â­Ã°Å¸Å½Â¶Ã¢ÂÂ¤', 'Ã°Å¸ËœÂ¤Ã°Å¸ËœÂ¤Ã°Å¸ËœÂ¤', 'Ã¢ÂÂ¤Ã°Å¸â€™â€¢', 'Ã°Å¸ËœÂ¶Ã°Å¸â„¢â€ž', 'Ã Â¥Â¥',
     'Ã°Å¸ËœÅ’Ã°Å¸ËœÅ’Ã°Å¸ËœÅ’Ã°Å¸ËœÅ’Ã°Å¸ËœÅ’Ã°Å¸ËœÅ’', 'Ã°Å¸â€™ÂÃ¢â‚¬ÂÃ¢â„¢â‚¬Ã¯Â¸ÂÃ°Å¸â€™ÂÃ¢â‚¬ÂÃ¢â„¢â‚¬Ã¯Â¸Â', 'Ã°Å¸ÂÂ¶', 'Ã°Å¸Ëœâ€šÃ°Å¸Ëœâ€šÃ°Å¸Ëœâ€šÃ°Å¸Ëœâ€š', '******', 'Ã°Å¸Ëœâ€¡', 'Ã°Å¸â€™Â',
     'Ã°Å¸Â¥Âº', 'Ã™Â¾Ã›Å’Ãšâ€ Ã›â€™', 'Ã°Å¸â€˜ÂÃ°Å¸ÂÂ½', 'Ã°Å¸Â¤â„¢', 'Ã Â¨Â¦Ã Â©â€¡', 'Ã°Å¸â€˜Â', 'ÃƒÂ¬', 'Ã°Å¸ËœÅ½Ã°Å¸â€Â¥', 'Ã°Å¸Ëœâ€¦Ã°Å¸Ëœâ€¦Ã°Å¸Ëœâ€¦', 'Ã°Å¸Â¤Å¾',
     'Ã Â¨â€ Ã Â¨Â§Ã Â¨Â¾Ã Â¨Â°Ã Â©Â', 'Ã°Å¸Ëœâ€¹Ã°Å¸Ëœâ€š', 'Ã°Å¸ËœÂ­Ã°Å¸â€˜Â', '!!!!!!!!!!!!!Ã¢â‚¬Â¦', 'Ã°Å¸â„¢ÂÃ°Å¸ÂÂ»', '!Ã°Å¸ËœÂÃ°Å¸ËœÂ', '!Ã¢â‚¬Â¦', 'Ã¢ÂÂ¤Ã°Å¸â€Â¥',
     'Ã°Å¸â€™Â¦Ã°Å¸â€™Â¦', 'Ã°Å¸â€™â€¹', '_Ã Â¤Â¸Ã Â¤Ë†Ã Â¤Â¯Ã Â¤Â¾', 'Ã°Å¸ËœËœÃ°Å¸ËœËœÃ°Å¸ËœËœÃ°Å¸ËœËœ', 'Ã Â¤Â¬Ã Â¤Â§Ã Â¤Â¾Ã Â¤â€¡Ã Â¤Â¯Ã Â¤Â¾Ã Â¤â€š', '-Ã¢â‚¬Â¦', 'Ã°Å¸Å’Â¸Ã¢ÂÂ¤Ã¯Â¸Â', 'Ã°Å¸ËœÂ³',
     'Ã°Å¸ËœÅ“Ã°Å¸ËœÅ“Ã°Å¸ËœÅ“Ã°Å¸ËœÅ“', 'Ã Â¤Â¦Ã Â¥â€¡Ã Â¤â€“Ã¢â‚¬Â¦', 'Ã Â¤Â¯Ã Â¥â€¡', 'Ã°Å¸â€¢â€¹Ã¢ËœÂªÃ¯Â¸Â', 'Ã°Å¸ËœÅ’..', '2019Ã°Å¸â€Â¥Ã°Å¸â€Â¥Ã°Å¸â€Â¥', 'Ã°Å¸Ëœâ€', 'Ã°Å¸ËœÂÃ°Å¸â„¢Å’',
     'Ã°Å¸â€¡ÂµÃ°Å¸â€¡Â°', 'Ã°Å¸Ëœâ€šÃ°Å¸Ëœâ€šÃ°Å¸Ëœâ€šÃ°Å¸Ëœâ€šÃ°Å¸Ëœâ€š', '...Ã°Å¸Ëœâ€¦Ã°Å¸Ëœâ€¦Ã°Å¸Â¤Â£Ã°Å¸Â¤Â£', 'Ã¢ÂÂ¤Ã¯Â¸Â', '..Ã¢â‚¬Â¦', 'Ã°Å¸ÂÂµÃ¯Â¸Â', 'Ã°Å¸â€Â¥Ã°Å¸Ëœâ€š',
     'Ã¢ÂÂ¤Ã¯Â¸ÂÃ°Å¸â€¡Â®Ã°Å¸â€¡Â³Ã°Å¸Å’Â¹', 'Ã°Å¸Å½â€°Ã°Å¸Å½â€°Ã°Å¸Å½â€°Ã°Å¸Å½â€°Ã°Å¸Å½â€°', '....Ã°Å¸Â¤Â¨', '..#', 'Ã°Å¸ËœËœÃ°Å¸ËœËœÃ°Å¸ËœËœ', 'Ã°Å¸Â¤Â©Ã¢Å“Å Ã¢Å“Å ', 'Ã¢â‚¬Ëœ', 'Ã°Å¸â„¢ÂÃ°Å¸â„¢Â',
     'Ã°Å¸â€˜Å½', 'Ã°Å¸ËœÂÃ°Å¸Ëœâ€š', 'Ã°Å¸â€™Å“..', 'Ã°Å¸Å½Â¶', 'Ã°Å¸ËœÂÃ°Å¸ËœÂÃ°Å¸ËœÂ', 'Ã°Å¸Â¤Â£Ã°Å¸Ëœâ€š', 'Ã°Å¸ËœÂÃ°Å¸Ëœâ€šÃ°Å¸Â¤Â£', 'Ã°Å¸â€˜â€ ', 'Ã°Å¸Å’Â²', 'Ã°Å¸ËœÂ©Ã°Å¸ËœÂ©',
     'Ã°Å¸Â¤â€”Ã°Å¸Â¤â€”Ã°Å¸Ëœâ€¦Ã°Å¸Ëœâ€¦Ã°Å¸â„¢â€ž', 'Ã°Å¸â„¢ÂÃ°Å¸â„¢ÂÃ°Å¸â„¢Â', 'Ã°Å¸ËœÅ½', 'Ã°Å¸Ëœâ€¡Ã°Å¸Ëœâ€¡', 'Ã Â¤Â®Ã Â¤Â¾Ã¢â‚¬Â¦', 'Ã°Å¸ËœÂÃ°Å¸Ëœâ„¢', 'Ã°Å¸â€˜ÂÃ°Å¸â€˜Â', '?Ã°Å¸Â¤â€”', 'Ã°Å¸Ëœâ€˜',
     'Ã‚ Ã¢â‚¬Â¦', 'Ã°Å¸Ëœâ€ Ã°Å¸Ëœâ€šÃ°Å¸Ëœâ€šÃ°Å¸Ëœâ€š', 'Ã˜ÂªÃ™Ë†Ã˜Â§Ã˜Â¸Ã˜Â¹Ã˜Â­', 'Ã¢ÂÂ¤Ã¯Â¸ÂÃ°Å¸ËœÂ', 'Ã°Å¸Ëœâ€˜Ã°Å¸Ëœâ€š', 'Ã°Å¸â€˜Å ', 'Ã¢â‚¬â€œ', 'Ã°Å¸ËœÂ­Ã°Å¸ËœÂ­Ã°Å¸ËœÂ­Ã°Å¸ËœÂ­',
     'Ã°Å¸â€˜ÂÃ°Å¸â€˜ÂÃ°Å¸â€˜Â', 'Ãšâ€ Ã™Ë†Ã˜Â±', 'Ã¢â‚¬Â.', '!!!Ã°Å¸Ëœâ‚¬Ã°Å¸Ëœâ‚¬Ã°Å¸Ëœâ€šÃ°Å¸Â¤Â£Ã°Å¸Ëœâ€š', 'Ã¢â‚¬â€', 'Ã›ÂÃ›â€™', 'Ã¢Å“â€¦', 'Ã¢Å“Å’Ã¯Â¸Â', 'Ã°Å¸â€“Â¤',
     '...Ã°Å¸â€˜Å’Ã°Å¸Ëœâ€šÃ°Å¸Ëœâ€¹', 'Ã°Å¸â€™Â¯Ã°Å¸â€™Â¯', 'Ã°Å¸ËœËœ', 'Ã°Å¸Ëœâ€°', 'Ã¢ÂÂ¤Ã¯Â¸ÂÃ°Å¸â€¡ÂµÃ°Å¸â€¡Â°', 'Ã¢ÂÂ¤Ã°Å¸â€™Å“', 'Ã°Å¸ËœÆ’Ã°Å¸ËœÆ’', 'Ã°Å¸â„¢ÂÃ°Å¸â€˜ÂÃ°Å¸ËœÂ',
     'Ã°Å¸Â¤Â£Ã°Å¸Â¤Â£Ã°Å¸Â¤Â£Ã°Å¸Â¤Â£Ã°Å¸Ëœâ€šÃ°Å¸Ëœâ€šÃ°Å¸Ëœâ€šÃ°Å¸Ëœâ€š', '?Ã°Å¸Ëœâ€ºÃ°Å¸ËœÅ“Ã°Å¸Ëœâ€ºÃ°Å¸ËœÅ“Ã°Å¸Ëœâ€ºÃ°Å¸ËœÅ“Ã°Å¸Ëœâ€ºÃ°Å¸ËœÅ“Ã°Å¸Ëœâ€º', 'Ã°Å¸ËœÂ­', 'Ã¢ÂÂ£Ã¯Â¸Â', '?Ã¢Ââ€',
     'Ã Â¤ÂµÃ Â¥ÂÃ Â¤Â¹Ã Â¥â‚¬Ã Â¤â€¢Ã Â¤Â²Ã¢â‚¬Â¦', 'Ã°Å¸â€™ÂªÃ°Å¸â€˜Å ', 'Ã°Å¸Ëœâ€º', 'Ã°Å¸â€¡ÂµÃ°Å¸â€¡Â°Ã°Å¸ÂÂ½Ã°Å¸ÂÂ·', 'Ã°Å¸ËœËœÃ°Å¸ËœËœÃ°Å¸ËœËœ.', '....Ã°Å¸Ëœ Ã°Å¸Ëœ', 'Ã°Å¸ËœÂ£', '...Ã°Å¸Â¤Å¾',
     'Ã°Å¸ËœÂ­Ã°Å¸ËœÂ­Ã°Å¸ËœÂ­Ã°Å¸ËœÂ­Ã°Å¸ËœÂ­Ã°Å¸ËœÂ­Ã°Å¸ËœÂ­Ã°Å¸ËœÂ­Ã°Å¸Ëœâ€šÃ°Å¸Ëœâ€šÃ°Å¸Ëœâ€š', 'Ã Â¤Â®Ã Â¥â€¡Ã Â¤Â°Ã Â¥â€¡', 'Ã¢ËœÂºÃ¯Â¸ÂÃ¢ËœÂºÃ¯Â¸Â', 'Ã°Å¸ËœÂ', 'Ã°Å¸â„¢ÂÃ°Å¸ÂÂ¾Ã°Å¸â€¡Â¿Ã°Å¸â€¡Â¦', '@Ã¢â‚¬Â¦',
     'Ã¢ÂÂ¤Ã¯Â¸ÂÃ¢Ëœâ‚¬Ã¯Â¸ÂÃ°Å¸â€™Å¡', 'Ã°Å¸â€˜Â»', '.Ã°Å¸â€™â€¢', 'Ã°Å¸Å’Â¼', '***', '...Ã°Å¸ËœÂ¡Ã°Å¸ËœÂ¡Ã°Å¸ËœÂ¡Ã°Å¸ËœÂ¡Ã°Å¸ËœÂ¡Ã°Å¸ËœÂ¡', 'Ã°Å¸â€Â', 'Ã°Å¸Ëœâ‚¬', '~^^', '@__',
     'Ã Â¤Â¯Ã Â¤Â¾Ã Â¤Â°', 'Ã°Å¸â„¢Æ’Ã°Å¸Â¤Â£', 'Ã¢ÂÂ¤Ã¯Â¸ÂÃ°Å¸â€™â€”', 'Ã°Å¸Å½Âµ', 'Ã°Å¸â€™Â¥Ã°Å¸â€™Â¥', 'Ã°Å¸Ëœâ€šÃ°Å¸ËœÂÃ°Å¸', 'Ã˜Â¬Ã™Ë†', 'ÃƒÂ¼', 'Ã¢ÂÂ¤Ã¯Â¸ÂÃ¢ÂÂ¤Ã¯Â¸Â',
     'Ã°Å¸ËœÂÃ¢ÂÂ¤Ã¯Â¸Â', 'Ã°Å¸ËœËœÃ°Å¸ËœËœÃ°Å¸ËœËœÃ°Å¸ËœËœÃ°Å¸ËœËœÃ°Å¸ËœËœÃ°Å¸ËœËœ', '...Ã°Å¸Ëœ Ã°Å¸ËœÂ­Ã°Å¸ËœÂ¢', '..Ã¢Å“Å’', '+Ã¢ÂÂ¤Ã¯Â¸Â', 'Ã°Å¸â€™â€¢',
     'Ã°Å¸Å½â€šÃ°Å¸Å½â€šÃ°Å¸Å½â€šÃ°Å¸Å½â€šÃ°Å¸Å½â€š', '.Ã°Å¸â„¢Ë†', 'Ã°Å¸Å½â€°', 'Ã°Å¸â€Â¥Ã°Å¸â€Â¥Ã°Å¸â€Â¥', 'Ã°Å¸ÂÂ°Ã°Å¸Å½Ë†Ã°Å¸Å½ÂÃ°Å¸Å½â€°', 'Ã°Å¸â€¡Â¿Ã°Å¸â€¡Â¦', '!!Ã°Å¸â€™ÂÃ°Å¸â€™Â',
     'Ã Â¤ÂµÃ Â¤â€šÃ Â¤Â¦Ã Â¥â€¡', 'Ã°Å¸â€™Å“Ã°Å¸â€™Å“Ã°Å¸ËœÂ­', '?Ã°Å¸ËœÅ¾', 'Ã°Å¸ËœÂ¬', '@@@', 'Ã°Å¸Ëœâ€°...', 'Ã°Å¸Ëœâ€šÃ°Å¸Ëœâ€šÃ¢â‚¬Â¦', 'Ã¢Å“Â¨Ã°Å¸â€™Â¥Ã°Å¸Å½Å Ã°Å¸Å½â€°', 'Ã°Å¸â€˜â€¦Ã°Å¸â€˜â€¦',
     'Ã›ÂÃ›â€™.', 'Ã°Å¸Â¥Â³Ã°Å¸Å½Å Ã°Å¸Å½â€°', 'Ã°Å¸Ëœâ€œ', 'Ã‚ @', 'Ã°Å¸ËœÆ’Ã°Å¸Ââ€™Ã°Å¸Ââ€™Ã°Å¸Ââ€™Ã°Å¸Ââ€™Ã°Å¸Ââ€™Ã°Å¸Ââ€™', 'Ã°Å¸â€¡Â®Ã°Å¸â€¡Â³Ã°Å¸â€¡Â®Ã°Å¸â€¡Â³', 'Ã°Å¸Å¡Â¨', 'Ã Â¤Â¸Ã Â¥â‚¬Ã Â¤â€“',
     'Ã›â€Ã›â€', 'Ã°Å¸Å’Å¸', 'Ã°Å¸Â¥Â³', 'Ã°Å¸â€˜ÂÃ°Å¸ËœÆ’', 'Ã°Å¸â€˜ÂÃ°Å¸â€¡ÂµÃ°Å¸â€¡Â°Ã°Å¸â€¡ÂµÃ°Å¸â€¡Â°Ã°Å¸â€¡ÂµÃ°Å¸â€¡Â°Ã°Å¸â€¡ÂµÃ°Å¸â€¡Â°Ã°Å¸â€¡ÂµÃ°Å¸â€¡Â°', 'Ã°Å¸â€˜â€°Ã°Å¸Å’Â¹Ã°Å¸Å’Â¹', 'Ã Â¨Â¦Ã Â¨Â¾Ã Â¨Â¤Ã Â¨Â¾',
     'Ã°Å¸ËœÂ¡Ã°Å¸ËœÂ¡', 'Ã°Å¸â€™â€¢Ã°Å¸â€™â€¢.', 'Ã¢Ââ€ž', '5Ã¢â‚¬Â¦', '.......Ã¢â‚¬Â¦', 'Ã°Å¸â€˜ÂÃ°Å¸ÂÂ¼Ã°Å¸Ëœâ€°', '370.Ã¢â‚¬Â¦', 'Ã°Å¸Ëœâ€¦Ã°Å¸Ëœâ€¦Ã°Å¸Ëœâ€¦Ã°Å¸Â¤Â£Ã°Å¸ËœÂÃ°Å¸ËœÂ',
     'Ã°Å¸ËœÅ Ã°Å¸ËœÅ ', 'Ã¢Ëœâ€˜', 'Ã¯Â¸ÂÃ¢Æ’Â£', 'Ã°Å¸Â¥Å ', '..Ã°Å¸Ëœâ€ ', '-@', '.Ã°Å¸Ëœâ€š', 'Ã¢Å“Å’', 'Ã°Å¸Ëœâ€šÃ°Å¸Ëœâ€šÃ°Å¸Ââ€ Ã°Å¸ÂÂÃ°Å¸Ââ€ Ã°Å¸ÂÂ', 'Ã°Å¸â„¢Å’Ã°Å¸Ëœâ€šÃ°Å¸Ëœâ€š',
     'Ã°Å¸â€™â€œ', 'Ã Â¤Å“Ã Â¤Â¯', 'Ã°Å¸Â¤â€”Ã°Å¸Â¤â€”Ã°Å¸Â¤â€”Ã°Å¸ËœËœÃ°Å¸ËœÂ', 'Ã Â¤â€ Ã Â¤ÂªÃ Â¤Â¨Ã Â¥â€¡Ã Â¥Â¤', 'Ã¢ËœÂºÃ¯Â¸ÂÃ¢ËœÂºÃ¯Â¸ÂÃ°Å¸â€™Å¾Ã°Å¸â€™Å¾', '.Ã°Å¸ËœÅ’Ã°Å¸ËœÅ ', '-Ã‚ ', 'Ã°Å¸â€˜â€¡Ã¢â‚¬Â¦',
     'Ã°Å¸â€˜â€¹', '...Ã°Å¸Å’ÂºÃ°Å¸â€™â€¢', '...Ã°Å¸ËœÂ±Ã°Å¸ËœÂ¨', 'Ã°Å¸â€™Â¥', 'Ã°Å¸â€˜Å’Ã°Å¸â€˜ÂÃ°Å¸ËœÅ Ã°Å¸Å’Â¹Ã°Å¸â„¢Â', 'Ã¢ÂÂ£Ã¯Â¸ÂÃ¢ÂÂ£Ã¯Â¸Â', 'Ã Â¤Â¹Ã Â¤Â¿Ã Â¤â€šÃ Â¤Â¦', 'Ã°Å¸â„¢Å’',
     '.....Ã°Å¸â€™â€Ã°Å¸â€™â€Ã°Å¸â€™â€', 'Ã°Å¸â€Â¥', 'Ã°Å¸â€¡Â§Ã°Å¸â€¡Â©', 'Ã°Å¸Â¥ÂºÃ¢ÂÂ¤Ã¯Â¸Â', 'Ã Â¤Â¹Ã Â¥â€šÃ Â¤Â', '!!Ã¢ÂÂ¤Ã¯Â¸ÂÃ¢ÂÂ¤Ã¯Â¸ÂÃ¢ÂÂ¤Ã¯Â¸Â', 'Ã°Å¸Ëœâ€š.',
     'Ã°Å¸â„¢ÂÃ°Å¸ÂÂ»Ã°Å¸â„¢ÂÃ°Å¸ÂÂ»', 'Ã Â¨Â¸Ã Â¨Â­Ã Â¨Â¸Ã Â©Ë†', 'Ã°Å¸Ëœâ€šÃ°Å¸Ëœâ€šÃ°Å¸Ëœâ€šÃ°Å¸Ëœâ€šÃ°Å¸Ëœâ€šÃ°Å¸Ëœâ€š', 'Ã°Å¸ËœÂ', 'Ã°Å¸Â¤â€”Ã°Å¸Â¤â€”', 'Ã°Å¸Â¤Â£Ã°Å¸Â¤Â£', 'Ã°Å¸Â¤Â£Ã°Å¸Â¤Â£Ã°Å¸Â¤Â£',
     'Ã°Å¸â€™â€”', 'Ã¢ËœÂ¹Ã¯Â¸ÂÃ¢ËœÂ¹Ã¯Â¸Â', 'Ã°Å¸Â¥Â°Ã°Å¸ËœÂ', '??Ã°Å¸Â¤â€Ã°Å¸Ëœâ€šÃ°Å¸Ëœâ€š', 'Ã°Å¸â„¢â€žÃ°Å¸â„¢â€ž', 'Ã°Å¸Ëœâ€šÃ°Å¸ËœË†', 'Ã°Å¸â€˜Â»Ã°Å¸â€˜Â»Ã°Å¸â€˜Â»', 'Ã Â¤Â¸Ã Â¥ÂÃ Â¤â€¦Ã Â¤Â°Ã Â¤Â¨Ã Â¥â‚¬',
     'Ã°Å¸ËœÂÃ°Å¸ËœÂÃ°Å¸ËœÂÃ°Å¸ËœÂ', '?Ã¢â‚¬Â¦', 'Ã°Å¸Â¤Â­', 'Ã°Å¸ËœÂ­Ã°Å¸ËœÂ­', 'Ã°Å¸ËœÂ­Ã°Å¸Ëœ', 'Ã°Å¸ËœÂÃ°Å¸â€Â¥', 'Ã°Å¸Ëœâ€šÃ°Å¸Ëœâ€šÃ°Å¸ËœÂÃ°Å¸ËœÂÃ°Å¸ËœÂ', 'Ã°Å¸Ëœâ€™', 'Ã°Å¸â€™Å¾',
     'Ã°Å¸â€™â€“', 'Ã°Å¸Ëœâ€.', 'Ã°Å¸Ëœâ€šÃ°Å¸Ëœâ€šÃ°Å¸Ëœâ€š', 'Ã£â€¦â€¹Ã£â€¦â€¹Ã£â€¦â€¹Ã£â€¦â€¹Ã£â€¦â€¹Ã£â€¦â€¹Ã£â€¦â€¹Ã£â€¦â€¹Ã£â€¦â€¹Ã£â€¦â€¹Ã£â€¦â€¹Ã£â€¦â€¹Ã£â€¦â€¹Ã£â€¦â€¹Ã£â€¦â€¹Ã¬Å¡Â°Ã«Â¦Â¬', 'Ã¢ÂÅ’', 'Ã°Å¸â€˜Å’', 'Ã Â¤Â¥Ã Â¥â‚¬',
     '.Ã¢ÂÂ¤Ã°Å¸â€˜Â¼', 'Ã‚ #', 'Ã°Å¸Ëœâ€šÃ°Å¸Ëœâ€š', 'Ã°Å¸â„¢Å’Ã°Å¸ÂÂ¾', 'Ã¢ÂÂ¤Ã¢ÂÂ¤', 'Ã°Å¸Ëœâ€žÃ°Å¸Ëœâ€ž', 'Ã°Å¸ËœË†', 'Ã‚Â©', '.Ã°Å¸Â¤Â§', '.Ã¢â‚¬Â', 'Ã°Å¸â„¢Æ’',
     'Ã°Å¸ËœÅ½Ã°Å¸ËœÅ½Ã°Å¸â€™Âª', 'ÃƒÂ¢Ã‚â‚¬Ã‚Â¢', 'Ã°Å¸â€™ÂÃ°Å¸Å’ÂºÃ°Å¸Å’Â¹Ã°Å¸Å’Â¸', 'Ã°Å¸Å’Å¾', 'Ã°Å¸â€˜Â¨Ã¢â‚¬ÂÃ°Å¸â€˜Â§Ã°Å¸ËœËœ', 'Ã°Å¸â„¢ÂÃ°Å¸â„¢ÂÃ°Å¸â„¢ÂÃ°Å¸â€˜ÂÃ¢Å¡ËœÃ¢Å¡ËœÃ°Å¸ËœÅ Ã°Å¸ËœÅ ', 'Ã°Å¸ËœÂ',
     'Ã°Å¸Â¤â€”Ã°Å¸Ëœâ€¡', 'Ã°Å¸â€Â¥Ã°Å¸â€Â¥Ã°Å¸â€˜Å’', 'Ã¢Ââ€', 'Ã°Å¸â€˜ÂÃ°Å¸â€™Â¯', 'Ã Â¤Â­Ã Â¤Â¾Ã Â¤Â°Ã Â¤Â¤', 'Ã¢Å“â€¦Ã¢Å“â€¦', 'Ã£â€¦â€¹Ã£â€¦â€¹Ã£â€¦â€¹Ã£â€¦â€¹Ã£â€¦â€¹Ã£â€¦â€¹Ã£â€¦â€¹Ã£â€¦â€¹Ã£â€¦â€¹',
     'Ã°Å¸â€˜Å’Ã¢â„¢Â¥Ã°Å¸â€Â¥Ã¢â‚¬Â¦', '.Ã°Å¸â€˜Â', '.....Ã°Å¸Ëœâ€šÃ°Å¸Â¤Â£Ã¢â‚¬Â¦', 'Ã°Å¸â„¢Å’Ã°Å¸ÂÂ»Ã°Å¸â„¢ÂÃ°Å¸ÂÂ»', 'Ã°Å¸Ëœâ€¢', 'Ã¢â‚¬â„¢', 'Ã°Å¸â€˜ÂÃ°Å¸ÂÂ»', 'Ã°Å¸ËœÂÃ°Å¸ËœËœ',
     'Ã¢Å“Å’Ã¯Â¸ÂÃ°Å¸â€™ÂÃ°Å¸ËœÅ Ã°Å¸Å’Â¹Ã°Å¸Å’Â¹Ã°Å¸â€¡Â®Ã°Å¸â€¡Â³', 'Ã°Å¸â€™Â©Ã°Å¸â€™Â©', 'Ã¢ÂÂ¤Ã¯Â¸ÂÃ°Å¸â„¢ÂÃ°Å¸ÂÂ»', 'Ã°Å¸Â¤ËœÃ°Å¸ÂÂ»', 'Ã¢â„¢Â¥Ã¢â„¢Â¥Ã¢â„¢Â¥Ã¢â„¢Â¥Ã¢â„¢Â¥Ã°Å¸â€™â€¢Ã°Å¸â€™â€¢',
     'Ã°Å¸Å½Å Ã°Å¸Å½â€°Ã°Å¸Å½Ë†', '#Ã¬â„¢â€¢ÃªÂµÂ°Ã­ËœÂ¸', 'Ã°Å¸ËœÂÃ°Å¸ËœÂÃ°Å¸ËœÂÃ°Å¸Å½â€šÃ°Å¸Å½ÂÃ°Å¸Å½ÂÃ°Å¸Å½Ë†Ã°Å¸Å½Ë†Ã°Å¸Å½â€ Ã°Å¸Å½â€ Ã°Å¸Å½â€°Ã°Å¸Å½Å Ã°Å¸ÂÂ°Ã°Å¸ÂÂ°',
     '.....Ã¢ÂÂ¤Ã°Å¸â€˜Å’Ã°Å¸â€˜Å’Ã°Å¸â€˜Å’Ã°Å¸â€˜Å’Ã°Å¸â€˜Å’Ã°Å¸â€˜Å’', 'Ã¢ÂÂ¤Ã¯Â¸ÂÃ¢ÂÂ¤Ã¯Â¸ÂÃ¢ÂÂ¤Ã¯Â¸ÂÃ¢ÂÂ¤Ã¯Â¸ÂÃ¢ÂÂ¤Ã¯Â¸ÂÃ¢ÂÂ¤Ã¯Â¸Â', 'Ã°Å¸Â¦Â¹Ã¢â‚¬ÂÃ¢â„¢â€šÃ¯Â¸Â', 'Ã Â¤Å“Ã Â¤Â¾Ã Â¤Â¤Ã Â¥â‚¬',
     'Ã°Å¸â€˜Â', '****', 'Ã°Å¸Ëœâ€¹', 'Ã°Å¸â€¡Â®Ã°Å¸â€¡Â³Ã°Å¸â€¡Â®Ã°Å¸â€¡Â³Ã°Å¸â€¡Â®Ã°Å¸â€¡Â³Ã°Å¸â€¡Â®Ã°Å¸â€¡Â³Ã°Å¸â€¡Â®Ã°Å¸â€¡Â³', 'Ã°Å¸Ëœâ€Ã°Å¸Ëœâ€', 'Ã°Å¸ÂÆ’', 'Ã°Å¸Ëœâ€ ', 'Ã°Å¸â€™â€', 'Ã°Å¸ËœÂÃ¢â‚¬Â¦',
     '??Ã°Å¸â„¢â€ž', 'Ã°Å¸ËœÂ©Ã°Å¸Ëœâ€˜', 'Ã°Å¸â€˜ÂÃ°Å¸â€˜ÂÃ°Å¸Ëœâ€°Ã°Å¸Ëœâ€°', '1Ã¢â‚¬Â¦', 'Ã°Å¸Ëœâ€ž', 'Ã¢ËœÂºÃ¢ËœÂº', 'Ã Â¤Â¹Ã Â¥â€¹..', 'Ã°Å¸ËœÂ¬Ã°Å¸ËœÂ¬..', 'Ã Â¨Å“Ã Â¨Â¿',
     'Ã°Å¸â€™â€œÃ°Å¸â€™â€œÃ°Å¸â€™â€œÃ°Å¸ËœÂÃ°Å¸ËœÂÃ°Å¸ËœÂ', 'Ã°Å¸Â¤Â£Ã°Å¸â€Â¥', 'Ã°Å¸Ëœâ€°Ã°Å¸Ëœâ€°Ã°Å¸Ëœâ€°', 'Ã°Å¸Ëœâ€˜Ã°Å¸Ëœâ€˜', 'Ã›Å’Ã›Â']

# Functions
def cleaning_punctuation(sentence):
    cleaned_tokens = []
    for token in sentence.split(' '):
        if any(punct in token for punct in punctuation1):
            print(token)
            continue
        elif token not in punctuation2:
            cleaned_tokens.append(token)
    return (' '.join(cleaned_tokens))


def cleaning_symbol(sentence):
    cleaned_tokens = []
    tokenlist = sentence.split(' ')
    previous = next_ = None
    l = len(tokenlist)
    for index, token in enumerate(tokenlist):
        if index > 0:
            previous = tokenlist[index - 1]
        if index < (l - 1):
            next_ = tokenlist[index + 1]
        if emoji_data_python.get_emoji_regex().findall(token):
            if token == previous:
                continue
            # if token == next_:
            #     print('found dup2:', token, next_)
            #     continue
            else:
                cleaned_tokens.append(token)
        cleaned_tokens.append(token)
    sentence = ' '.join(cleaned_tokens)
    cleaned_tokens = []

    for token in sentence.split(' '):
        # replaces words for any symbols from dictionary
        if token in symbol_dict:
            cleaned_tokens.append(symbol_dict[token])

        # Checks for emojis and replaces with short names
        if emoji_data_python.get_emoji_regex().findall(token):
            unified = emoji_data_python.char_to_unified(token)
            emojis = unified.split('-')
            emojis = list(dict.fromkeys(emojis))
            for emoji in emojis:
                for index, key in enumerate(emoji_data_python.emoji_short_names):
                    if emoji in emoji_data_python.emoji_short_names[key].__dict__.values():
                        description = key.replace('_', '-').split('-')
                        # print(description)
                        for word in description:
                            cleaned_tokens.append(word)
                        continue
        elif token not in symbol:
            cleaned_tokens.append(token)
    return (' '.join(cleaned_tokens))


def english_remove_stop_words(example_sent):
    word_tokens = word_tokenize(example_sent)
    filtered_sentence = [w for w in word_tokens if not w in stop_words]
    filtered_sentence = []
    for w in word_tokens:
        if w not in stop_words:
            filtered_sentence.append(w)
    return (' '.join(filtered_sentence))


def hindi_remove_stop_words(example_sent):
    word_tokens = word_tokenize(example_sent)
    filtered_sentence = [w for w in word_tokens if not w in hin_stop_words]
    filtered_sentence = []
    for w in word_tokens:
        if w not in stop_words:
            filtered_sentence.append(w)
    return (' '.join(filtered_sentence))


def stemming(sentence):
    wordList = nltk.word_tokenize(sentence)
    stem_words = [snowBallStemmer.stem(word) for word in wordList]
    return (' '.join(stem_words))

# print(emoji_data_python.emoji_short_names['gear'].__dict__)

# Iterate through files, clean and convert to .tsv
for file in files:

    filepath = Path(__file__).resolve().parents[1]
    filepath = str(filepath)
    datapath = filepath + '/' + 'data' + '/' + 'hindi-english' + '/' + file
    # outpath = filepath / 'processed_data' / 'hindi-english' / title

    data = pd.read_csv(datapath + '.txt', sep="\t", header=None, engine='python', quoting=csv.QUOTE_NONE)

    s1 = []
    s2 = []
    s3 = []
    number = 0
    empty = 0
    null = 0

    for i in range(len(data)):
        if data[0][i] == "meta":
            s3 = " ".join(s1)
            s2.append(s3)
            s1 = []
            number += 1
        elif data[0][i] == "@":
            continue
        elif data[0][i - 1] == "@":
            continue
        elif data[0][i] == '':
            empty += 1
        elif data[0][i] == '"':
            continue
        elif data[0][i] == "'":
            continue
        elif (pd.isnull(data[0][i])):
            null += 1
            continue
        else:
            s1.append(str(data[0][i]))
    s2.append(" ".join(s1))
    s2.pop(0)
    print(file, 'total tweets: ', number)
    print(file, 'empty datapoints: ', empty)
    print(file, 'null datapoints:  ', null)

    sentiment = []
    label = []

    if 'test' not in file:
        for i in range(len(data)):
            if data[2][i] == 'negative':
                label.append(0)
                sentiment.append("negative")
            elif data[2][i] == 'neutral':
                label.append(1)
                sentiment.append("neutral")
            elif data[2][i] == 'positive':
                label.append(2)
                sentiment.append("positive")

    # s4 = []
    for i in range(len(s2)):
        if re.search("http[.]*", s2[i]) != None:
            s2[i] = s2[i][:re.search("http[.]*", s2[i]).span()[0]]
        else:
            continue

    print('starting')
    s4 = []

    with alive_bar(len(s2)) as bar:
        for i in range(len(s2)):
            s4.append(cleaning_symbol(s2[i]))
            bar()
    print('symbol cleaning complete')

    s5 = []
    for i in range(len(s4)):
        s5.append(cleaning_punctuation(s4[i]))

    s6 = []
    for i in range(len(s5)):
        s6.append((english_remove_stop_words(s5[i])))

    s7 = []
    for i in range(len(s6)):
        s7.append(hindi_remove_stop_words(s6[i]))

    id = []
    for i in range(len(data)):
        if data[1][i] != 'Eng' and data[1][i] != 'Hin' and data[1][i] != 'O' and data[1][i] != 'EMT' and data[1][i] != 'nan':
            id.append(data[1][i])
        else:
            continue

    clean_id = []
    for i in range(len(id)):
        if type(id[i]) == str:
            clean_id.append(id[i])

    with open(datapath + ".csv", 'wt', encoding="utf-8") as out_file:
        tsv_writer = csv.writer(out_file)
        if 'test' in file:
            tsv_writer.writerow(['id', 'sentence'])
        elif 'test' not in file:
            tsv_writer.writerow(['id', 'sentence', 'label', 'sentiment'])
        for k in range(len(s7)):
            if len(s5[k]) > 4:
                f = []
                f.append(clean_id[k])
                f.append(s5[k])
                if 'test' not in file:
                    f.append(label[k])
                    f.append(sentiment[k])
                tsv_writer.writerow(f)
            else:
                continue

    with open(datapath + "_no_sw" + ".csv", 'wt', encoding="utf-8") as out_file:
        tsv_writer = csv.writer(out_file)
        if 'test' in file:
            tsv_writer.writerow(['id', 'sentence'])
        elif 'test' not in file:
            tsv_writer.writerow(['id', 'sentence', 'label', 'sentiment'])
        for k in range(len(s7)):
            if len(s7[k]) > 4:
                f = []
                f.append(clean_id[k])
                f.append(s7[k])
                if 'test' not in file:
                    f.append(label[k])
                    f.append(sentiment[k])
                tsv_writer.writerow(f)
            else:
                continue